# GANs (Generative Adversarial Networks) - README

## Introduction to GANs

Generative Adversarial Networks (GANs) are a type of machine learning model used to generate realistic data samples, such as images or text. GANs consist of two neural networks that compete with each other:

- **Generator (G)**: Creates fake data from random noise.
- **Discriminator (D)**: Evaluates whether a given data sample is real (from the dataset) or fake (produced by the Generator).

## How GANs Work

The core idea of GANs is a game between the Generator and the Discriminator. The Generator tries to produce data that is indistinguishable from real data, while the Discriminator tries to correctly classify data as real or fake.

### Minimax Game

The training of GANs is formulated as a **minimax game**. Here’s a simplified explanation of how it works:

- **Discriminator’s Objective**: The Discriminator wants to maximize its ability to correctly identify real and fake data.
- **Generator’s Objective**: The Generator wants to minimize the Discriminator’s ability to identify fake data, effectively trying to fool the Discriminator.

The **minimax** formula captures this adversarial process:

- **Discriminator Loss**: Measures how well the Discriminator can distinguish between real and fake data. We want the Discriminator to maximize this value.

  LossD​=−[Ex∼pdata​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))]]

  Where:
  - Ex∼pdata​(x) is the expectation over real data \(x\).
  - Ez∼pz​(z) is the expectation over noise \(z\).
  - D(x) is the probability that the Discriminator classifies \(x\) as real.
  - G(z) is the data generated from noise \(z\).

- **Generator Loss**: Measures how well the Generator is at producing data that the Discriminator thinks is real. The Generator wants to minimize this value, effectively maximizing \( D(G(z)) \).

  LossG​=−Ez∼pz​(z)​[logD(G(z))]

  Where:
  - The Generator aims to maximize \( D(G(z)) \), making the Discriminator more likely to classify generated samples as real.

### Why Use Logarithms?

Logarithms are used in the loss functions because they:

- **Ensure Smooth Gradients**: Logarithms provide smoother gradients, which helps in stable and effective training.
- **Measure Likelihood**: Logarithms are related to probabilities, making it easier to compute the likelihood of real versus fake data.

**Without Logarithm**

Consider the following loss components:

    For real data: D(x)D(x), where D(x)D(x) is the Discriminator's output probability that real data xx is real.
    For fake data: 1−D(G(z))1−D(G(z)), where G(z)G(z) is the data generated by the Generator from random noise zz.

Without using the logarithm, the gradient with respect to the Discriminator's output would simply depend on the difference between these probabilities and their desired values (1 for real, 0 for fake). However, this can lead to gradients that do not provide enough incentive to improve when the model is confident (close to 1 or 0) but wrong.

**With Logarithm**

Using the log function changes the loss function to:

    For real data: log⁡D(x)logD(x)
    For fake data: log⁡(1−D(G(z)))log(1−D(G(z)))

The logarithm compresses values, especially near 0 and 1. This means:

    When D(x)D(x) is close to 1 (correct for real data), log⁡D(x)logD(x) is close to 0, and the gradient is small, indicating minimal change needed.
    When D(x)D(x) is close to 0 (wrong for real data), log⁡D(x)logD(x) becomes a large negative number, leading to a large gradient, indicating a need for significant correction.

Similarly, for fake data:

    When 1−D(G(z))1−D(G(z)) is close to 1 (correct for fake data), log⁡(1−D(G(z)))log(1−D(G(z))) is close to 0, and the gradient is small.
    When 1−D(G(z))1−D(G(z)) is close to 0 (wrong for fake data), log⁡(1−D(G(z)))log(1−D(G(z))) becomes a large negative number, leading to a large gradient.

### Understanding \( D(x) \) and \( 1 - D(x) \)

- **\( D(x) \)**: Represents the probability that the Discriminator believes the data \( x \) is real. The Discriminator is trained to maximize this probability for real data.
- **\( 1 - D(x) \)**: Represents the probability that the Discriminator believes the data \( x \) is fake. The Discriminator is trained to minimize this probability for real data.

The Generator’s goal is to increase \( D(G(z)) \), making the Discriminator more likely to classify generated samples as real.


## Types of GANs

1. **Vanilla GAN**: The original GAN model with basic architecture and training procedure.
2. **Conditional GAN (cGAN)**: Generates data based on additional conditions or labels.
3. **Deep Convolutional GAN (DCGAN)**: Utilizes convolutional networks for improved image generation.
4. **Wasserstein GAN (WGAN)**: Measures the difference between real and generated data using Wasserstein distance for better training stability.
5. **Wasserstein GAN with Gradient Penalty (WGAN-GP)**: Enhances WGAN with a gradient penalty to stabilize training.
6. **CycleGAN**: Performs image-to-image translation without paired examples.
7. **StyleGAN**: Focuses on generating high-quality images with controllable styles and features.

Explored each type of GAN with in Pytorch.

**References**

Ian Goodfellow, et al. Generative Adversarial Networks. NIPS 2014 
